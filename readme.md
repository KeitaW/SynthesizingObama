I have made report for the lecture in notes/report.ipynb.
In this repo, I have tried to synthesize lip motion from audio data using an simplified version of method featured in https://grail.cs.washington.edu/projects/AudioToObama/ .
Below shows an example synthesis.

This is the original lip motion.

![](notes/actual_lip.gif)

This is the synthesized lip motion.

![](notes/searched_lip.gif)

I also make the original implementation (https://github.com/supasorn/synthesizing_obama_network_training) Python 3, Tensorflow 1.4.1 compatible (run.py, util.py).
